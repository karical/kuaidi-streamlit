import streamlit as st
from PIL import Image
import os
import io
import numpy as np
import cv2
import tempfile
import torch

from backend import (
    load_uvdoc_model, unwarp_img,
    load_yolo_model, plot_results,
    perform_ocr,run_cnocr_visualization
)

# å¸¸é‡
OCR_PROMPT_DEFAULT = "è¯·è¯†åˆ«å¿«é€’å•ä¸Šçš„å…³é”®ä¿¡æ¯ï¼Œä¾‹å¦‚å¿«é€’å•å·ã€å¯„ä»¶äººã€æ”¶ä»¶äººåœ°å€ç­‰ã€‚"
IMG_SIZE = [488, 712]

st.set_page_config(layout="wide")
st.title("ğŸ“„å›¾ç‰‡å¤„ç†ä¸ä¿¡æ¯æå–")

# 1ï¸âƒ£ é€‰æ‹©å›¾ç‰‡
if "top3_image_paths" not in st.session_state or not st.session_state.top3_image_paths:
    st.warning("è¯·å…ˆåœ¨ä¸»é¡µé¢è¿è¡Œæ£€æµ‹å¹¶ç”Ÿæˆå›¾ç‰‡ã€‚")
    st.stop()

cols = st.columns(4)
selected_index = st.session_state.get("selected_image_index", None)
for i, p in enumerate(st.session_state.top3_image_paths):
    with cols[i]:
        img = Image.open(p)
        st.image(img, use_container_width=True, caption=f"å›¾ç‰‡ {i+1}")
        if st.button(f"é€‰æ‹©å›¾ç‰‡ {i+1}", key=i):
            st.session_state.selected_image_index = i
            selected_index = i

with cols[3]:
    if selected_index is None:
        st.info("è¯·é€‰æ‹©å·¦ä¾§å›¾ç‰‡è¿›è¡Œå¤„ç†")
        st.stop()
    path = st.session_state.top3_image_paths[selected_index]
    sel_img = Image.open(path)
    st.image(sel_img, use_container_width=True, caption="å·²é€‰å›¾ç‰‡")

    # ä¿å­˜åˆ° session
    buf = io.BytesIO()
    sel_img.save(buf, format="PNG")
    st.session_state.selected_img_bytes = buf.getvalue()
    arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)
    st.session_state.cv2_selected_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)

# 2ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹
with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹..."):
    if "unwrap_model" not in st.session_state:
        st.session_state.unwrap_model = load_uvdoc_model("./weights/unwrap_model/best_model.pkl",map_location=torch.device("cpu"))

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("æ¨¡å‹é…ç½®")
    sel_unwrap = st.selectbox(
        "å‡ ä½•æ ¡æ­£æ¨¡å‹",
        os.listdir("./weights/unwrap_model"),
        index=0
    )
    sel_yolo = st.selectbox(
        "åˆ†ç±»æ£€æµ‹æ¨¡å‹",
        os.listdir("./weights/cls_model"),
        index=0
    )
    # åˆ‡æ¢æ ¡æ­£æ¨¡å‹
    if st.session_state.get("current_unwrap") != sel_unwrap:
        st.session_state.unwrap_model = load_uvdoc_model(
            os.path.join("./weights/unwrap_model", sel_unwrap)
        )
        st.session_state.current_unwrap = sel_unwrap
    ocr_prompt = st.text_area("OCR æç¤ºè¯", OCR_PROMPT_DEFAULT)

st.markdown("---")

# 3ï¸âƒ£ åŸå›¾ã€æ£€æµ‹ã€æ ¡æ­£å¸ƒå±€
col1, col2, col3 = st.columns(3)

with col1:
    st.subheader("åŸå§‹å›¾ç‰‡")
    st.image(st.session_state.selected_img_bytes, use_container_width=True)

with col2:
    st.subheader("æ–‡æ¡£æ£€æµ‹")
    if st.button("è¿è¡Œæ£€æµ‹"):
        fd, tmp_path = tempfile.mkstemp(suffix=".png")
        os.close(fd)
        cv2.imwrite(tmp_path, st.session_state.cv2_selected_img)
        ymodel = load_yolo_model(os.path.join("./weights/cls_model", sel_yolo))
        results = ymodel(tmp_path)
        os.remove(tmp_path)
        ann = plot_results(st.session_state.cv2_selected_img, results)
        st.session_state.annotated_img = ann
    if "annotated_img" in st.session_state:
        st.image(
            st.session_state.annotated_img,
            channels="BGR",
            use_container_width=True,
            caption="æ£€æµ‹ç»“æœ"
        )

with col3:
    st.subheader("æ ¡æ­£å›¾åƒ")
    if st.button("è¿è¡Œæ ¡æ­£"):
        fd2, tmp_path2 = tempfile.mkstemp(suffix=".png")
        os.close(fd2)
        cv2.imwrite(tmp_path2, st.session_state.cv2_selected_img)
        proc_path = unwarp_img(
            os.path.join("./weights/unwrap_model", sel_unwrap),
            tmp_path2,
            IMG_SIZE,
            st.session_state.unwrap_model
        )
        os.remove(tmp_path2)
        st.session_state.processed_img = proc_path
    if "processed_img" in st.session_state:
        st.image(
            st.session_state.processed_img,
            use_container_width=True,
            caption="æ ¡æ­£åå›¾åƒ"
        )


#ocréƒ¨åˆ†ï¼Œæ”¯æŒåœ¨çº¿cpu
st.subheader("OCR å¯è§†åŒ–ç»“æœ")
if st.button("è¿è¡Œ OCR å¯è§†åŒ–"):
    # ç¡®å®šè¾“å…¥å›¾åƒè·¯å¾„
    input_path = st.session_state.get("processed_img", None)
    if input_path is None:
        # ä¸´æ—¶ä¿å­˜åŸå›¾
        fd, tmp = tempfile.mkstemp(suffix=".png")
        os.close(fd)
        cv2.imwrite(tmp, st.session_state.cv2_selected_img)
        input_path = tmp

    # è°ƒç”¨åç«¯å‡½æ•°
    vis_img, texts, boxes = run_cnocr_visualization(
        img_fp         = input_path,
        rec_model_fp   = "./weights/ocr_model/ocr_densenet.onnx",
        det_model_fp   = "./weights/ocr_model/ch_PP-OCRv4_det_infer.onnx",
        font_path      = "./fonts/simhei.ttf",
        font_size      = 20
    )

    # è‹¥å†™äº†ä¸´æ—¶æ–‡ä»¶ï¼Œåˆ é™¤ä¹‹
    if 'tmp' in locals():
        os.remove(tmp)

    # ä¸‰åˆ—å±•ç¤ºï¼šå¯è§†åŒ–å›¾ / æ–‡å­— / åæ ‡
    c1, c2, c3 = st.columns(3)
    # è½¬ RGB
    rgb = cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB)
    with c1:
        st.image(rgb, use_container_width=True, caption="OCR å¯è§†åŒ–")
    with c2:
        st.markdown("**è¯†åˆ«æ–‡å­—**")
        for i, t in enumerate(texts,1):
            st.write(f"{i}. {t}")
    with c3:
        st.markdown("**æ–‡å­—åæ ‡**")
        for i, b in enumerate(boxes,1):
            x0,y0 = b[0]; x2,y2 = b[2]
            st.write(f"{i}. ({x0},{y0})â†’({x2},{y2})")
# # è§†è§‰æ¨¡å‹éƒ¨åˆ†ï¼Œå¦‚æœæ˜¯åœ¨çº¿éœ€è¦æ¥å…¥api
# st.markdown("---")
# st.subheader("è§†è§‰æ¨¡å‹")
# if st.button("è¿è¡Œ"):
#     if "processed_img" in st.session_state:
#         ocr_res = perform_ocr(ocr_prompt, st.session_state.processed_img)
#         st.session_state.ocr_result = ocr_res
# if "ocr_result" in st.session_state:
#     st.text_area("æ¨¡å‹è¿”å›ç»“æœ", st.session_state.ocr_result, height=200)

st.markdown(
    "<p style='text-align: center; color: gray;'>Â© 2025 è®¡ç®—æœºè§†è§‰è¯¾ç¨‹è®¾è®¡ Â· Authored by ä¸‡åŠ±ä¸º</p>",
    unsafe_allow_html=True
)